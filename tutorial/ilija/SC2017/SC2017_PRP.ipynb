{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling SC2017 PRP productions\n",
    "\n",
    "This module has functions to:\n",
    "- create all the workload\n",
    "- move from state to state [created,training,trained,generating,generated,transporting,done]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from time import time, sleep\n",
    "from elasticsearch import Elasticsearch, exceptions as es_exceptions\n",
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'],timeout=60)\n",
    "index_name='sc2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -XPOST \"atlas-kibana.mwt2.org:9200/_template/sc2017\" -d @SC2017_template.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [--nb-epochs NB_EPOCHS] \n",
    " [--batch-size BATCH_SIZE]\n",
    " [--latent-size LATENT_SIZE] \n",
    " [--disc-lr DISC_LR]\n",
    " [--gen-lr GEN_LR] \n",
    " [--adam-beta ADAM_BETA] \n",
    " [--prog-bar]\n",
    " [--no-attn] \n",
    " [--debug] \n",
    " [--d-pfx D_PFX] \n",
    " [--g-pfx G_PFX]\n",
    " dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workload():\n",
    "    \n",
    "    # clean all\n",
    "    try:\n",
    "        es.indices.delete(index='sc2017')\n",
    "    except:\n",
    "        print(\"not there?\")\n",
    "    \n",
    "    id=0\n",
    "    \n",
    "    l0 = [50,100,200]\n",
    "    l1 = [0.0001,0.0002,0.0005]\n",
    "    l2  = [0.00001, 0.00002, 0.00005]\n",
    "    l3    = ['', '--no-attn']\n",
    "    l4    = ['gamma.yaml', 'eplus.yaml', 'pion.yaml']\n",
    "    for a in l0:\n",
    "        for b in l1:\n",
    "            for c in l2:\n",
    "                for d in l3:\n",
    "                    for e in l4:\n",
    "                        doc={}\n",
    "                        doc['created']=int(time()*1000)\n",
    "                        doc['status']='created'\n",
    "                        doc['training']={\n",
    "                            'nb-epochs' : a,\n",
    "                            'disc-lr' : b,\n",
    "                            'gen-lr' : c,\n",
    "                            'particle' : e,\n",
    "                            'output_folder':'/data/CaloGAN/weights/'+str(id)\n",
    "                        }\n",
    "                        if d!='': doc['training']['options'] = d\n",
    "                        doc['generator']={\n",
    "                            'input_folder':'/data/CaloGAN/weights/'+str(id), \n",
    "                            'output_folder':'/data/CaloGAN/outputs/'+str(id), \n",
    "                            'epochs' : a,\n",
    "                            'sets' : 10, \n",
    "                            'showers' : 100000\n",
    "                        }\n",
    "                        doc['transferring_options']=['root://faxbox.usatlas.org:1094//faxbox2/user/ivukotic/outputs/'+str(id)]\n",
    "#                         print(doc)\n",
    "                        es.create(index=index_name, doc_type='doc', id=id, body=doc)\n",
    "                        id+=1\n",
    "\n",
    "\n",
    "def get_training_job():\n",
    "    my_query={\n",
    "        \"size\": 1,\n",
    "        \"query\":{ \n",
    "            \"term\": {\"status\":\"created\"} \n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=index_name, body=my_query )\n",
    "    res=res['hits']\n",
    "    if res['total']==0:\n",
    "        print('no training jobs at the moment.')\n",
    "        return\n",
    "    res_id=res['hits'][0]['_id']\n",
    "    res=res['hits'][0]['_source']\n",
    "    \n",
    "    res['status']='training'\n",
    "    res['start_training']=int(time()*1000)\n",
    "    es.update(index=index_name, doc_type='doc', id=res_id, body={\"doc\":res})\n",
    "    return (res_id, res)\n",
    "\n",
    "def get_generating_job():\n",
    "    my_query={\n",
    "        \"size\": 1,\n",
    "        \"query\":{ \n",
    "            \"term\": {\"status\":\"trained\"} \n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=index_name, body=my_query )\n",
    "    res=res['hits']\n",
    "    if res['total']==0:\n",
    "        print('no generating jobs at the moment.')\n",
    "        return\n",
    "    res_id=res['hits'][0]['_id']\n",
    "    res=res['hits'][0]['_source']\n",
    "    \n",
    "    res['status']='generating'\n",
    "    res['start_generating']=int(time()*1000)\n",
    "    es.update(index=index_name, doc_type='doc', id=res_id, body={\"doc\":res})\n",
    "    return (res_id, res)\n",
    "\n",
    "def get_transfering_job():\n",
    "    my_query={\n",
    "        \"size\": 1,\n",
    "        \"query\":{ \n",
    "            \"term\": {\"status\":\"generated\"} \n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=index_name, body=my_query )\n",
    "    res=res['hits']\n",
    "    if res['total']==0:\n",
    "        print('no transfering jobs at the moment.')\n",
    "        return\n",
    "    res_id=res['hits'][0]['_id']\n",
    "    res=res['hits'][0]['_source']\n",
    "    \n",
    "    res['status']='transfering'\n",
    "    res['start_transfering']=int(time()*1000)\n",
    "    es.update(index=index_name, doc_type='doc', id=res_id, body={\"doc\":res})\n",
    "    return (res_id, res)\n",
    "\n",
    "def done_training(id):\n",
    "    res={'status':'trained', 'end_training':int(time()*1000) }\n",
    "    es.update(index=index_name, doc_type='doc', id=id, body={\"doc\":res})\n",
    "\n",
    "def done_generating(id):\n",
    "    res={'status':'generated', 'end_generating':int(time()*1000) }\n",
    "    es.update(index=index_name, doc_type='doc', id=id, body={\"doc\":res})\n",
    "\n",
    "def done_transfering(id):\n",
    "    res={'status':'transfered', 'end_transfering':int(time()*1000) }\n",
    "    es.update(index=index_name, doc_type='doc', id=id, body={\"doc\":res})\n",
    "\n",
    "def set_status(id, new_status):\n",
    "    res={'status':new_status }\n",
    "    es.update(index=index_name, doc_type='doc', id=id, body={\"doc\":res})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flow():\n",
    "    \n",
    "    create_workload()\n",
    "\n",
    "    (id, job) = get_training_job()\n",
    "    print(id, job)\n",
    "    sleep(5)\n",
    "    done_training(id)\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    (id, job) = get_generating_job()\n",
    "    print(id, job)\n",
    "    sleep(5)\n",
    "    done_generating(id)\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    (id, job) = get_transfering_job()\n",
    "    print(id, job)\n",
    "    sleep(5)\n",
    "    done_transfering(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    if len(sys.argv)!=2:\n",
    "        print('Usage - sc2017.py <creator|trainer|generator|transporter>')\n",
    "    else:\n",
    "        print( 'this pod will be:', sys.argv[1] )\n",
    "    \n",
    "    role=sys.argv[1]\n",
    "    if role=='creator':\n",
    "        create_workload()\n",
    "    elif role=='trainer':\n",
    "        while (True):\n",
    "            res = get_training_job()\n",
    "            if not res:\n",
    "                print('waiting...')\n",
    "                sleep(120)\n",
    "                continue\n",
    "            (id, job) = res\n",
    "            print('training job:',id, '\\nsetting up:\\n', job)\n",
    "            op=job['training']\n",
    "            \n",
    "            print ('(re)create output directory')\n",
    "            output = subprocess.check_output(['rm', '-rf', op['output_folder']])\n",
    "            output = subprocess.check_output(['mkdir', '-p', op['output_folder']])\n",
    "\n",
    "            options=[]\n",
    "            options.append( '--output_folder=' + op['output_folder'] )\n",
    "            options.append( '--nb-epochs=' + str(op['nb-epochs']) )\n",
    "            options.append( '--disc-lr=' + str(op['disc-lr']) )\n",
    "            options.append( '--gen-lr=' + str(op['gen-lr']) )\n",
    "            if 'options' in op: \n",
    "                options.append( op['options'] )\n",
    "            options.append( op['particle'] )\n",
    "            print(options)\n",
    "            output = subprocess.check_output(['/ML_platform_tests/tutorial/sc2017_prp/train.py'] + options)\n",
    "            print(output)\n",
    "            done_training(id)\n",
    "            sleep(15)\n",
    "    elif role=='generator':\n",
    "        while (True):\n",
    "            res = get_generating_job()\n",
    "            if not res:\n",
    "                print('waiting...')\n",
    "                sleep(120)\n",
    "                continue\n",
    "            (id, job) = res\n",
    "            print('generator job:',id, '\\nsetting up:\\n', job)\n",
    "            g=job['generator']\n",
    "            output = subprocess.check_output(['rm', '-rf', g['output_folder']])\n",
    "            output = subprocess.check_output(['mkdir', '-p', g['output_folder']])\n",
    "            options=[g['input_folder'], g['output_folder'], str(g['epochs']), str(g['sets']), str(g['showers']) ]\n",
    "            print(options)\n",
    "            output = subprocess.check_output(['/ML_platform_tests/tutorial/sc2017_prp/generator.py']+options)\n",
    "            print(output)\n",
    "            done_generating(id)\n",
    "            sleep(15)\n",
    "    elif role=='transporter':\n",
    "        while (True):\n",
    "            res = get_transfering_job()\n",
    "            if not res:\n",
    "                print('waiting...')\n",
    "                sleep(120)\n",
    "                continue\n",
    "            (id, job) = res\n",
    "            print('transporter job:',id, '\\nsetting up:\\n', job)\n",
    "            output_folder=job['generator']['output_folder']\n",
    "            output = subprocess.check_output( ['xrdcp', '-r', output_folder, job['transferring_options'] ] )\n",
    "            print(output)\n",
    "            done_transfering(id)\n",
    "            sleep(60)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_statut(0,\"created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
